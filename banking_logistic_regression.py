# -*- coding: utf-8 -*-
"""Banking-Logistic-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rk5CkOQV0TcgoQZu6QVXU2No-m9DIQtv

##Introduction to the Banking Dataset Classification
##Overview
####The Banking Dataset Classification is a comprehensive dataset sourced from the banking industry, primarily focused on predicting customer behavior based on a range of socio-economic attributes. This dataset is widely used for classification tasks in data science and machine learning projects, particularly for building predictive models to identify potential customers for banking products.

##Source
####This dataset is available on Kaggle, a well-known platform for data science competitions and datasets. The specific dataset used here is often referred to as the "Bank Marketing" dataset, which is publicly accessible and commonly utilized for educational and research purposes.

##Dataset Description
####The dataset contains information about a marketing campaign conducted by a Portuguese banking institution. The goal of the campaign was to predict whether a client would subscribe to a term deposit based on various attributes.

##Features
####The dataset comprises several features that capture demographic, socio-economic, and campaign-specific information. Here's a brief overview of the primary features:

####1) Age: The age of the client.
####2) Job: Type of job (e.g., admin., blue-collar, entrepreneur).
####3) Marital: Marital status (e.g., married, single, divorced).
####4) Education: Level of education (e.g., secondary, tertiary, unknown).
####5) Default: Whether the client has credit in default (yes or no).
####6) Balance: The average yearly balance in the clientâ€™s account.
####7) Housing: Whether the client has a housing loan (yes or no).
####8) Loan: Whether the client has a personal loan (yes or no).
####9) Contact: Type of communication contact (e.g., cellular, telephone).
####10) Day: Last contact day of the month.
####11) Month: Last contact month of the year.
####12) Duration: Duration of the last contact in seconds.
####13) Campaign: Number of contacts performed during this campaign.
####14) Pdays: Number of days since the client was last contacted from a previous campaign.
####15) Previous: Number of contacts performed before this campaign.
####16) Poutcome: Outcome of the previous marketing campaign (e.g., success, failure).

##Target Variable
####y: The target variable indicates whether the client subscribed to a term deposit (yes or no).

##Objective
####The primary objective of analyzing this dataset is to build a predictive model that can accurately classify whether a customer will subscribe to a term deposit. This involves:

####Exploratory Data Analysis (EDA) to understand the distribution and relationships within the data.
####Data Preprocessing to handle missing values, categorical variables, and feature scaling.
####Model Building using various classification algorithms such as Logistic Regression, Decision Trees, Random Forest, etc.
####Model Evaluation to assess the performance of the predictive model using metrics like accuracy, precision, recall, and F1-score.

##Significance
####The Banking Dataset Classification project not only provides insights into customer behavior but also helps in developing targeted marketing strategies. By accurately predicting customer responses, banks can optimize their campaigns, reduce costs, and improve customer engagement.

##Usage
####This dataset is particularly useful for practicing classification techniques and improving machine learning skills. It is also a valuable resource for understanding real-world applications of predictive modeling in the banking sector.

Source:
https://www.kaggle.com/datasets/rashmiranu/banking-dataset-classification/data
"""

import pandas as pd #Pandas: This library is widely used for data manipulation and analysis
import numpy as np #NumPy: This library is fundamental for numerical computing in Python.

df_train = pd.read_csv("new_train.csv") #line reads a CSV file into a DataFrame named df using the pandas library.
df_test = pd.read_csv("new_test.csv")

df_train.head() #We will now read the data from a CSV file into a Pandas DataFrame Let us have a look at how our dataset looks like using df.head()

df_test.head() #We will now read the data from a CSV file into a Pandas DataFrame Let us have a look at how our dataset looks like using df.head()

df_train.columns #Displays the names of the columns from the dataset.

df_test.columns #Displays the names of the columns from the dataset.

df_train.shape # Displays the total count of the Rows and Columns respectively.

df_test.shape # Displays the total count of the Rows and Columns respectively.

# Ensuring the test dataset has all columns present in the training dataset
# Add missing columns with a default value (e.g., zero)
missing_cols = set(df_train.columns) - set(df_test.columns)
for col in missing_cols:
    df_test[col] = 0

df_train.shape  #Re-checking the number of the rows and columns of the dataset.

df_test.shape #Re-checking the number of the rows and columns of the dataset.

df_train.isnull().sum() #Displays the total count of the null values in the particular columns.

df_test.isnull().sum() #Displays the total count of the null values in the particular columns.

df_train.info()

df_test.info()

from sklearn.preprocessing import LabelEncoder
#This imports the LabelEncoder class, which is used for converting categorical data into numerical data. This transformation is required because logistic regression, require numerical input.

categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'y']
#Here, I've listed all the categorical columns in the dataset. These are columns containing data in non-numeric form, such as strings or categories, which need to be encoded.

label_encoders = {}
#This dictionary will store a LabelEncoder object for each categorical column, allowing us to access the encoders later if needed (e.g., for inverse transformation).

for column in categorical_columns:
    label_encoders[column] = LabelEncoder()
    df_train[column] = label_encoders[column].fit_transform(df_train[column])

#Loop through Categorical Columns: For each column in categorical_columns, a new LabelEncoder object is created and stored in the label_encoders dictionary.
#Fit and Transform: The fit_transform method is applied to the column, which fits the encoder to the unique values in the column and transforms these values into numerical labels.
#The transformed data replaces the original categorical data in the df_train DataFrame.

df_train.head() #checking the dataframe with the encoded values.

# Separate features and target variable from the training data
X_train = df_train.drop('y', axis=1)
y_train = df_train['y']

# The test data does not have the target variable 'y'
# Ensure test data is ordered correctly without 'y'
X_test = df_test[X_train.columns]

print(X_train.isnull().sum())
print(X_test.isnull().sum())

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()
#create an instance of StandardScaler. This object will be used to scale your training and test data.

# Fit and transform the training data
X_train_scaled = scaler.fit_transform(X_train)
#Fit: The fit method calculates the mean and standard deviation for each feature in the training data (X_train). These statistics are used to scale the data.
#Transform: The transform method applies the scaling to the training data, producing X_train_scaled. This ensures that the training data has a mean of 0 and a standard deviation of 1 for each feature.

# Transform the test data
X_test_scaled = scaler.transform(X_test)
#Here, we apply the scaling to the test data (X_test) using the same parameters (mean and standard deviation) calculated from the training data.
#This is important to maintain consistency in how the data is scaled.

"""##Why Scaling is Important

###1) Normalization of Features: Features with different scales can unduly influence the model's training process.
###2) Improving Model Performance: Logistic regression and other machine learning algorithms that use gradient descent optimization can benefit significantly from feature scaling. Scaling ensures that the optimization algorithm converges faster and finds a better solution.
###3) Consistency: By scaling both the training and test data, we ensure that the model's performance evaluation is fair and consistent. It's crucial that the test data is transformed using the same scaling parameters as the training data to avoid introducing bias.

"""

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train_scaled, y_train)

"""###LogisticRegression(): This function creates an instance of the logistic regression model. Logistic regression is a statistical method used for binary classification problems, predicting the probability of a binary outcome based on one or more input features.
###max_iter=1000: The max_iter parameter sets the maximum number of iterations that the algorithm will use to converge to a solution. Sometimes, logistic regression models may not converge within the default number of iterations (100). Increasing max_iter ensures that the optimization algorithm has enough iterations to find the best solution.

###fit(X_train_scaled, y_train): This method trains the logistic regression model using the scaled training data (X_train_scaled) and the corresponding target labels (y_train). During this process, the model learns the relationship between the features and the target variable, estimating the coefficients that define this relationship.

##By following these steps, we're preparing the logistic regression model to make accurate predictions based on the patterns it has learned from the training data. Once trained, this model can be used to predict outcomes for new, unseen data.
"""

# Predict the target variable for the test dataset
y_test_pred = model.predict(X_test_scaled)

# Create a DataFrame for the predictions
df_test['y_pred'] = y_test_pred


print(df_test.head())

"""###model.predict(X_test_scaled): This method uses the trained logistic regression model to predict the target variable for the test data (X_test_scaled). The predict method outputs the predicted class labels (0 or 1 in the case of binary classification) based on the learned relationships from the training phase.

###df_test['y_pred']: This line creates a new column in your test DataFrame (df_test) called y_pred, which stores the predicted class labels. By adding this column, you can easily compare the predicted values (y_pred) with the actual target values (y_test) for evaluation and analysis.

##Why These Steps Are Important
###1) Prediction:
###Making predictions on the test data is a crucial step in the model evaluation process. It allows us to assess how well the model generalizes to new, unseen data.

###2) Storing Predictions:
###Adding the predictions to the DataFrame provides a convenient way to compare the predicted and actual values. This is useful for creating visualizations, calculating evaluation metrics, and understanding the model's performance in detail.

###3) Model Evaluation:
###With the predictions stored in the DataFrame, we can easily calculate various evaluation metrics, such as accuracy, precision, recall, F1 score, and confusion matrix. These metrics help you understand the strengths and weaknesses of your model and guide any necessary adjustments or improvements.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Predict on the training data
y_train_pred = model.predict(X_train_scaled)
#model.predict(X_train_scaled): This method uses the logistic regression model, which was trained on X_train_scaled, to predict the target variable for the same training data.
#The output, y_train_pred, contains the predicted class labels for each instance in the training dataset.


# Evaluate the model
accuracy = accuracy_score(y_train, y_train_pred) #This function computes the accuracy of the model, which is the ratio of correctly predicted instances to the total number of instances
precision = precision_score(y_train, y_train_pred) #Precision is the ratio of true positive predictions to the total predicted positives. It measures the accuracy of positive predictions.
recall = recall_score(y_train, y_train_pred) #Recall is the ratio of true positive predictions to the total actual positives. It measures the model's ability to correctly identify all positive instances.
f1 = f1_score(y_train, y_train_pred) #f1_score(y_train, y_train_pred): The F1 score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall, providing a single metric that captures both aspects.
#Interpretation: The F1 score is particularly useful when you need a balance between precision and recall, especially in cases of class imbalance.

print(f'Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# Confusion Matrix
conf_matrix = confusion_matrix(y_train, y_train_pred)
print('Confusion Matrix:')
print(conf_matrix)

"""##This particular model provides the Accuracy of 90.48%

###The confusion matrix typically has the following format:

* True Negatives (TN): The number of instances correctly predicted as negative.
* False Positives (FP): The number of instances incorrectly predicted as positive (Type I error).
* False Negatives (FN): The number of instances incorrectly predicted as negative (Type II error).
* True Positives (TP): The number of instances correctly predicted as positive

###Metrics Calculation:

###The values from the confusion matrix are used to calculate various performance metrics, such as precision, recall, specificity, and the F1 score, providing a comprehensive evaluation of the model.

###The confusion matrix is a foundational tool in classification analysis, providing a clear and concise summary of the model's performance and areas for potential improvement.
"""

